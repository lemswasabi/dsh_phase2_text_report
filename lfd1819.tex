% packages {{{
% File lfd1617.tex
%
%% Based on the style files for EACL-2017
%% Based on the style files for ACL-2016
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks]{hyperref}

\usepackage{pdflscape}
\hypersetup{
    colorlinks,
    linkcolor={red},
    linktoc=page,
    citecolor={blue},
    urlcolor={blue}
}

\usepackage{caption}
\captionsetup[table]{font={stretch=1.2}}
\captionsetup[figure]{font={stretch=1.2}}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle, language=Python}

%%%% LEAVE THIS IN
\eaclfinalcopy

\graphicspath{{./figures/}}

\newcommand\BibTeX{B{\sc ib}\TeX}% }}}
  
% {{{ title
\title{DSH - Sentiment Analysis}
%Add the authors name alphabetically!
\author{Leminh Nguyen
\textsuperscript{1}, Alex Poldrugo\textsuperscript{2}, Rafidison Santatra Rakotondrasoa\textsuperscript{3},\\
%\vspace{0pt}
\\
\textsuperscript{1}0180531722,  \textsuperscript{2} Your student number \textsuperscript{3} your student number \\
\texttt{\{le.nguyen.001, second name, \& third name\}@student.uni.lu} \\ %in case your adding Lu email address use this template and if you want to add different email address from your coauthor use the following template. 
% \texttt{nnn.xzx23@yahoo.com}\\
% \texttt{xxx.yyy@yahoo.com}
}
\date{\date}  

\pagenumbering{roman}% }}}

\begin{document}

\maketitle

% abstract {{{
\begin{abstract}
  \textbf{During this project, we classify product reviews according to sentiments and the topic of the reviews. We achieved for sentiment and topic classification testing accuracies of 0.833\% and 0.929\% respectively.}
\end{abstract}
% }}}

% Introduction {{{
\section{Introduction}

% Problem statement

In this study, we are working with a dataset of reviews. Our goal is to classifiy the review samples according to their sentiment, which can be \textit{negative} or \textit{positive}. Additionally, we classifiy these samples to the topic of the reviews: \textit{books}, \textit{camera}, \textit{dvd}, \textit{health}, \textit{music} or \textit{software}.

% We use the modules in scikit-learn to split data into training and testing sets. And we train the data and estimate how well it is likely to perform on out-of-sample data through evaluation functions such as precision, recall, f-score, and also with confusion matrix. And the distinct values of prior and posterior probabilities also shows how important is the additional information taken into account. 
% }}}

% Data {{{
\section{Data}

\textcolor{red}{Alex}

In this section, you need to describe your dataset in details. 
% }}}

% EDA {{{
\section{Exploratory Data Analysis (EDA)}

In this section, we investigate data trend, to explore the data and visualize it.

\subsection{Loading of data}

Before we can explore the dataset and visualize it, we first need to transform the dataset in to a representation that we can work with. The given dataset is stored it a \textit{.txt} file. In Fig. \ref{fig:dataset_txt}, we visualized the structure of the .txt file. Each row of this file contains a review sample which is delimited by a newline character.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{dataset_txt.png}
    \caption{Preview of the .txt file containing the review samples.}
    \label{fig:dataset_txt}
\end{figure}

Each sample is divided into sections. The first word represents the topic of the review, whereas the second word depicts its sentiment. The third word of the sample serves as id. The rest of the row presents the review text itself. This is visualized in Fig. \ref{fig:review_sample}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{review_sample.png}
    \caption{Layout of a review sample.}
    \label{fig:review_sample}
\end{figure}

Each review text is already tokenized, therefore we only need to convert the dataset into a representation we can work with. We chose to transform the dataset into a Pandas DataFrame with the function defined in listing \ref{lst:load_data}.

\begin{lstlisting}[label={lst:load_data}, caption=Transformation of the dataset.]
def trainset_to_df(path):
    """
    trainset_to_df converts trainset.txt to a pandas dataframe
    Args:
        path: path string to trainset.txt
    Return:
        df: pandas dataframe of trainset.txt
    """

    with open(path, 'r') as f:
        lines = f.readlines()

    lines = [[line.split()[0], line.split()[1], line.split()[2], ' '.join(line.split()[3:])] for line in lines]
    df = pd.DataFrame(lines, columns=['topic', 'sentiment', 'id', 'text'])

    return df
\end{lstlisting}

The function in \ref{lst:load_data} removes any newline or trailing white characters and divides the sample into specific columns: labels, sentiment, id and text.

\subsection{Visualization of data}

\textcolor{red}{Alex}

% In this section, you need to investigate data trend, to explore the data and visualize it. The most important part of understanding the data is identifying the questions that you want to answer and then the second important part is to summarize their main characteristics, often plotting them visually. The plotting in EDA consists of Histograms, Box plot, Scatter plot, and many more. 
% }}}

% Models and approach {{{
\section{Method/Approach/Model}

\subsection{Data pre-processing}

We load our dataset with our \textit{trainset\_to\_df()} function defined in \ref{lst:load_data}:

\begin{lstlisting}
reviews = trainset_to_df('trainset.txt')
\end{lstlisting}

First of all, we check if there are any Nan or missing values inside the dataframe:

\begin{lstlisting}
reviews.isnull().values.any()
\end{lstlisting}

This returns us with \textit{False}, which means we don't have any missing values in the dataset.

As a next step, we verify if any duplicate \textit{id} values exists:

\begin{lstlisting}
reviews['id'].duplicate().any()
# True
reviews['id'].duplicate().value_counts
# True = 5000, False = 1000
\end{lstlisting}

This tells us that we have 5000 duplicate review ids. In order to verify if the duplicate id values and the text values are dependent we sort the dataframe according to the id value as seen in Fig. \ref{fig:id_sort}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{id_sort.png}
    \caption{First samples of reviews sorted by \textit{id}.}
    \label{fig:id_sort}
\end{figure}

There is no relation between the id and text columns, therefore we can remove the id column since it won't be useful as a feature:

\begin{lstlisting}
reviews = reviews[['topic', 'sentiment', 'text']]
\end{lstlisting}

When checking for duplicates in the text column:

\begin{lstlisting}
reviews[reviews['text'].duplicated()].sort_values('text')
\end{lstlisting}

We did not find any duplicates in this column.

After dealing with missing and duplicate values, we continue with binarizing and label encode our variables. First we start to binarize our categorical varible in the sentiment column:

\begin{lstlisting}
reviews['sentiment'] = reviews['sentiment'].apply(lambda sentiment: 0 if sentiment == 'neg' else 1)
\end{lstlisting}

This operation applies the given lambda along the column where it maps \textit{'neg'} string to $0$ and \textit{'pos'} to $1$.

Then we label encode the topic column with \textit{LabelEncoder()} from the \textit{sklearn.preprocessing} library:

\begin{lstlisting}
from sklearn import preprocessing

le = preprocessing.LabelEncoder()
review['topic'] = le.transform(reviews['topic'])

\end{lstlisting}

\subsection{Text Representation}

\textcolor{red}{Santatra}

\subsection{Feature Selection}

\textcolor{red}{Santatra}

\subsection{Implementation}

We compile every preprocessing and feature engineering steps into a function called preprocess\_dataset. This function executes every previously mentioned steps such as removing stopwords from the reviews, binarizing and encoding the respective labels. Additionally, it vectorizes the features with the TFID representation and uses the chi\_square\_test to select features.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{data_preprocessing_function.png}
    \caption{Definition of \textit{preprocess\_dataset()} function.}
    \label{fig:preprocess_dataset}
\end{figure}

\subsection{Performance Measures}

\paragraph{Classification predictions.} In binary classification, an estimator can two types of predictions: \textit{True} or \textit{False}. True predictions can be either a true positive or a true negative where a model correctly predicts the positive or negative class respectively. The same works for the false positive and false negative predictions where the model makes incorrect predictions.

\paragraph{Conclusion matrix.} An error matrix which visualizes the performance of an algorithm. Each row of the matrix represents the samples of an actual class, whereas the columns represent the samples in a predicted class. Fig. \ref{fig:confusion_matrix} visualizes an example of a confusion matrix.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{confusion_matrix.jpg}
    \caption{An example of a confusion matrix.}
    \label{fig:confusion_matrix}
\end{figure}

\paragraph{Accuracy.} A metrix to evaluation the classification of a model. It is defined by the ratio of correct predictions made by the model.

\begin{equation}
    Accuracy = \frac{Number \ of \ correct \ predictions}{Total \ number \ of \ predictions}
\end{equation}

\paragraph{Precision.} Represents the ratio of positive classifications which are actually correct amon the samples classified as positive.

\begin{equation}
    Precision = \frac{TP}{TP + FP}
\end{equation}

\paragraph{Recall.} Represents the ratio of actual positives which were identified correctly among the total number of positive examples.

\begin{equation}
    Recall = \frac{TP}{TP + FN}
\end{equation}

\paragraph{F-score.} This metric combines the precision and recall and is defined as a harmonic mean of these two measures.

\begin{equation}
    F_1 = \frac{TP}{TP + \frac{1}{2}(FP + FN)}
\end{equation}

\paragraph{Cross-validation.} Another metric to assess the performance of a classifier is the cross-validation approach. It assesses how a classifier generalizes on an independent data set. With this approach, we are able to detect overfitting of the model and mitigate biased slection of the training and testing subsets. This method has the following process:

\begin{itemize}
    \item Split the dataset into $k$ subsets
    \item Rotate the training and validation subsets to create $k$ independent trained classifiers
    \item Take the mean of the scores of these $k$ classifiers to assess the overall accuracy performance
\end{itemize}

A visualization of K-Fold, an example of cross-validation, is given is Fig. \ref{fig:cross_validation}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{cross_validation.png}
    \caption{K-fold example where $k = 5$.}
    \label{fig:cross_validation}
\end{figure}

\subsection{Baseline: Training of a Naive Bayes model}

% For this section, we need to have a clear overview of your method so try to be precise and explain it in a short form that someone else can understand it easily. 
% If you want to use any math for this section, there is a software that if you give it a picture of your math, you will get the latex code for that math :). The name of the software is \textbf{Mathpix snip}.

\subsubsection{Baseline: Topic classifiction}

\textcolor{red}{Alex}

\subsubsection{Baseline: Sentiment classifiction}

\textcolor{red}{Santatra}

\subsubsection{Performance evaluation with punctuation}

\paragraph{Sentiment classifiction.}

In this section, we show case the test accuracies for the sentiment classifiction. In Fig. \ref{tab:sentiment_without_punctuation} we trained the models without including punctuations, whereas in Fig. \ref{tab:sentiment_with_punctuation} we included them during the training.

\begin{table}[!ht]
\centering
\caption{Sentiment classifiction test accuracy scores without punctuation.}
\label{tab:sentiment_without_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.822 \\ \hline
MultinomialNB & 0.807 \\ \hline
RandomForestClassifier & 0.800 \\ \hline
KNeighborsClassifier & 0.743 \\ \hline
DecisionTreeClassifier & 0.687 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Sentiment classifiction test accuracy scores with punctuation.}
\label{tab:sentiment_with_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.825 \\ \hline
MultinomialNB & 0.813 \\ \hline
RandomForestClassifier & 0.804 \\ \hline
KNeighborsClassifier & 0.732 \\ \hline
DecisionTreeClassifier & 0.663 \\ \hline
\end{tabular}
%
}
\end{table}

\paragraph{Topic classifiction.}

In this section, we show case the test accuracies for the topic classifiction. In Fig. \ref{tab:topic_without_punctuation} we trained the models without including punctuations, whereas in Fig. \ref{tab:topic_with_punctuation} we included them during the training.

\begin{table}[ht]
\centering
\caption{Topic classifiction test accuracy scores without punctuation.}
\label{tab:topic_without_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.919 \\ \hline
MultinomialNB & 0.916 \\ \hline
KNeighborsClassifier & 0.896 \\ \hline
RandomForestClassifier & 0.880 \\ \hline
DecisionTreeClassifier & 0.798 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[ht]
\centering
\caption{Topic classifiction test accuracy scores with punctuation.}
\label{tab:topic_with_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
MultinomialNB & 0.923 \\ \hline
SVC	& 0.919 \\ \hline
KNeighborsClassifier & 0.889 \\ \hline
RandomForestClassifier & 0.885 \\ \hline
DecisionTreeClassifier & 0.797 \\ \hline
\end{tabular}
%
}
\end{table}

During this analysis, we observed that the overall performance of the models improve when including the punctuation in the review text when training the models.

\subsection{Ensemble methods}

\textcolor{red}{Santatra}

\subsection{Grid Search}

Grid search is an approach to find the optimal hyperparameters of a model which yields the most accurate predictions. Hyperparameter is a characteristics of an estimator which cannot be learned from data. These hyperparameters have to be set before training. During grid search we try to find the best hyperparameters values through an exhaustive search over specified parameter values for an estimator.

For this study, we implemented a grid search framework which iterates over possible estimator candidates and executes grid search for each classifer while considering the defined hyperparameter search space.

\subsubsection{Grid Search: Base classifiers}

We executed grid search for both sentiment and topic classification tasks. The best hyperparameters for the sentiment and topic classification task can be consulted in Table \ref{tab:sentiment_gs} and in Table \ref{tab:topic_gs} respectively. The \textit{best\_score} in these tables represent the training accuracies.

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the sentiment classifiction task.}
\label{tab:sentiment_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_params & best\_score & std\_test\_score \\ \hline
SVC	& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\}	& 0.824	& 0.022 \\ \hline
MultinomialNB & \{'alpha': 10\}	& 0.807	& 0.031 \\ \hline
RandomForestClassifier & \{'class\_weight': None, 'criterion': 'entropy', 'n\_estimators': 70\}	& 0.802	& 0.014 \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 85, 'p': 2\}	& 0.742	& 0.017 \\ \hline
DecisionTreeClassifier & \{'class\_weight': 'balanced', 'criterion': 'entropy', 'splitter': 'best'\}	& 0.692	& 0.024 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the topic classifiction task.}
\label{tab:topic_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_params & best\_score & std\_test\_score \\ \hline
SVC	& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\}	& 0.919	& 0.003 \\ \hline
MultinomialNB & \{'alpha': 1\}	& 0.917	& 0.011 \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 95, 'p': 2\}	& 0.890	& 0.008 \\ \hline
RandomForestClassifier & \{'class\_weight': 'balanced\_subsample', 'criterion': 'gini', 'n\_estimators': 90\}	& 0.881	& 0.008 \\ \hline
DecisionTreeClassifier & \{'class\_weight': None, 'criterion': 'gini', 'splitter': 'random'\}	& 0.800	& 0.023 \\ \hline
\end{tabular}
%
}
\end{table}

\subsubsection{Grid Search: Ensemble methods}

\textcolor{red}{Santatra}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the sentiment classifiction task.}
\label{tab:sentiment_em_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_params & best\_score & std\_test\_score \\ \hline
SVC	& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\}	& 0.824	& 0.022 \\ \hline
MultinomialNB & \{'alpha': 10\}	& 0.807	& 0.031 \\ \hline
RandomForestClassifier & \{'class\_weight': 'None', 'criterion': 'entropy', 'n\_estimators': 70\}	& 0.802	& 0.014 \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 85, 'p': 2\}	& 0.742	& 0.017 \\ \hline
DecisionTreeClassifier & \{'class\_weight': 'balanced', 'criterion': 'entropy', 'splitter': 'best'\}	& 0.692	& 0.024 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the topic classifiction task.}
\label{tab:topic_em_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_params & best\_score & std\_test\_score \\ \hline
SVC	& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\}	& 0.822	& 0.015 \\ \hline
MultinomialNB & \{'alpha': 1\}	& 0.807	& 0.013 \\ \hline
RandomForestClassifier & \{'class\_weight': 'balanced\_subsample', 'criterion': 'gini', 'n\_estimators': 70\}	& 0.800	& 0.005 \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 95, 'p': 2\}	& 0.743	& 0.01 \\ \hline
DecisionTreeClassifier & \{'class\_weight': None, 'criterion': 'entropy', 'splitter': 'random'\}	& 0.687	& 0.005 \\ \hline
\end{tabular}
%
}
\end{table}

% }}}

% Results and discussion {{{
\section{Results \& Discussion}

This is an important section in your report. Try to spend more time on this section. 

\subsection{Results}

\begin{table}[!ht]
\centering
\caption{The caption needs to be short and informative\label{tab:sample}}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|l|l|l|l|}
\hline
             & precision & recall & f1-score & support \\ \hline
least        & 0.85      & 0.69   &          &         \\ \hline
left         & 0.68      & 0.79   &          &         \\ \hline
left-center  & 0.65      & 0.78   &          &         \\ \hline
right        & 0.99      & 0.85   &          &         \\ \hline
right-center & 0.44      & 0.07   &          &         \\ \hline
avg / total  & 0.79      & 0.77   &          &         \\ \hline
\end{tabular}
%
}
\end{table}

% You can split \verb!Results! and \verb!Discussion! sections. Use the possibility of adding table \ref{tab:sample} and figure \ref{fig:boxplot} to make your report to be easier to follow for readers! 
% You can use following websites to make your latex table:

% \begin{itemize}
%     \item {\href{https://www.latex-tables.com/}{The first suggestion}}
%     \item {\href{https://www.tablesgenerator.com/}{The second suggestion}}
% \end{itemize}

% Figure \ref{fig:boxplot} is a sample of a figure which you can use in your report. 

% \begin{figure}[!hbtp]
%   \centering
%   \includegraphics[width = 1\linewidth, height = 0.5\linewidth]{boxplot_age_ADS.png}
%   \caption{\scriptsize Box plot of participants age.\label{fig:boxplot}}
% \end{figure}

\subsection{Discussion}

% }}}

% Conclusion {{{
\section{Conclusion}

% Summary your findings in one or two paragraphs. If you have any reference to support your work, you can also add them to your report \cite{gerven1997comparative}. 
% This is a template for your report, please submit the pdf file for your report at the end. 
% The name of pdf file should be:

We presented our work on sentiment analysis for the \textit{text} project from phase 2 of the DSH course. During this project, we try to classifiy product reviews to a sentiment or a review topic. We achieved for sentiment and topic classification test accuracies of 0.833\% and 0.929\% respectively.

These test accuracies were obtained by the stacking classifiers. We discussed which type of classifiers should be selected and concluded that it depends on the setting in which it will be used.
% }}}

% Table of collaborations {{{
\section{Table of collaborations}

You will find the task descriptions executed by each group member in Table \ref{tab:collab} in the Appendix.

% In this section, we want you briefly describe what each one did in your group for the assigned project (Table \ref{tab:collab}).
% }}}

% Appendix {{{
\onecolumn
\newpage
\clearpage
\section{Appendix}
% }}}

% Task descriptions {{{
\begin{table}[!h]
\centering
\caption{Task descriptions executed by each student of the group.}\label{tab:collab}
\begin{tabular}{|c| >{\centering\arraybackslash}m{0.6\textwidth}|}\hline

\textbf{Group member}  & \textbf{Task} \\

\hline

\multirow{2}{*}{\textbf{Leminh Nguyen}}

                  & Helping out with implementing the data visualization \\\cline{2-2}
                  & Implementation of dataset conversion to Pandas DataFrame representation \\\cline{2-2}
                  & Implementation of data pre-processing (Data cleaning, selection, normalization) \\\cline{2-2}
                  & Define performance measure + cross validation \\\cline{2-2}
                  & Definition of Grid Search and implementation of grid search framework to iterate over a set of classifer-hyperparameters candidates \\\cline{2-2}
                  & Performance evaluation with punctuation \\\cline{2-2}
                  & Performance evaluation without punctuation \\\cline{2-2}
                  & Sentiment Grid search benchmark \\\cline{2-2}
                  & Topic Grid search benchmark \\\cline{2-2}
                  % & Final benchmark assessment \\\cline{2-2}
                  % & Best model selection \\\cline{2-2}
                  % & Formulate abstract, introduction, discussion and conclusion sections \\\cline{2-2}
                  & Formulate abstract, introduction conclusion sections \\\cline{2-2}
                  & Format the report template \\

\hline

\multirow{2}{*}{\textbf{Alex Poldrugo}}

                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\

\hline

\multirow{2}{*}{\textbf{Rafidison Rakotondrasoa}}

                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\

\hline

\end{tabular}
\end{table}
% }}}

% Comments {{{
% \section{Pdf}
% Please save your file as a pdf file and apply the following template for naming your pdf file.
% \textit{DSH\_nameofyourproject\_phasenumber}
% \noindent Good luck :)
% }}}

% Bibliography {{{
\bibliographystyle{apalike}
\raggedright
{\scriptsize
\bibliography{ref.bib}}
% }}}

\end{document}
