% packages {{{
% File lfd1617.tex
%
%% Based on the style files for EACL-2017
%% Based on the style files for ACL-2016
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks]{hyperref}

\usepackage{pdflscape}
\hypersetup{
    colorlinks,
    linkcolor={red},
    linktoc=page,
    citecolor={blue},
    urlcolor={blue}
}

\usepackage{caption}
\captionsetup[table]{font={stretch=1.2}}
\captionsetup[figure]{font={stretch=1.2}}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle, language=Python}

%%%% LEAVE THIS IN
\eaclfinalcopy

\graphicspath{{./figures/}}

\newcommand\BibTeX{B{\sc ib}\TeX}% }}}
  
% {{{ title
\title{DSH - Sentiment Analysis}
%Add the authors name alphabetically!
\author{Leminh Nguyen
\textsuperscript{1}, Alex Poldrugo\textsuperscript{2}, Rafidison Santatra Rakotondrasoa\textsuperscript{3},\\
%\vspace{0pt}
\\
\textsuperscript{1}0180531722,  \textsuperscript{2} Your student number \textsuperscript{3} your student number \\
\texttt{\{le.nguyen.001, second name, \& third name\}@student.uni.lu} \\ %in case your adding Lu email address use this template and if you want to add different email address from your coauthor use the following template. 
% \texttt{nnn.xzx23@yahoo.com}\\
% \texttt{xxx.yyy@yahoo.com}
}
\date{\date}  

\pagenumbering{roman}% }}}

\begin{document}

\maketitle

% abstract {{{
\begin{abstract}
  \textbf{During this project, we classify product reviews according to sentiments and the topic of the reviews. We achieved for sentiment and topic classification testing accuracies of 0.833\% and 0.929\% respectively.}
\end{abstract}
% }}}

% Introduction {{{
\section{Introduction}

% Problem statement

In this study, we are working with a dataset of reviews. Our goal is to classifiy the review samples according to their sentiment, which can be \textit{negative} or \textit{positive}. Additionally, we classifiy these samples to the topic of the reviews: \textit{books}, \textit{camera}, \textit{dvd}, \textit{health}, \textit{music} or \textit{software}.

% We use the modules in scikit-learn to split data into training and testing sets. And we train the data and estimate how well it is likely to perform on out-of-sample data through evaluation functions such as precision, recall, f-score, and also with confusion matrix. And the distinct values of prior and posterior probabilities also shows how important is the additional information taken into account. 
% }}}

% Data {{{
\section{Data}%Alex
Our data represents reviews for certain products. The reviews have been categorised by sentiment and topic. The sentiment is divided in two values, negative and positive. The topic is divided in 6 different types of products, books, camera, dvd, health, music, software. The data has also an id for each review, which represents the name of the text file the review comes from. And finally we have the text category, which is the textual comment part of the review.\newline
We have in total 6000 reviews in our dataset. Each row of our data represents a review and the column represent each the topic, sentiment, id and text.  

%In this section, you need to describe your dataset in details. 
% }}}

% EDA {{{
\section{Exploratory Data Analysis (EDA)}

In this section, we investigate data trend, to explore the data and visualize it.

\subsection{Loading of data}

Before we can explore the dataset and visualize it, we first need to transform the dataset in to a representation that we can work with. The given dataset is stored in a \textit{.txt} file. In Fig. \ref{fig:dataset_txt}, we visualized the structure of the .txt file. Each row of this file contains a review sample which is delimited by a newline character.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{dataset_txt.png}
    \caption{Preview of the .txt file containing the review samples.}
    \label{fig:dataset_txt}
\end{figure}

Each sample is divided into sections. The first word represents the topic of the review, whereas the second word depicts its sentiment. The third word of the sample serves as id. The rest of the row presents the review text itself. This is visualized in Fig. \ref{fig:review_sample}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{review_sample.png}
    \caption{Layout of a review sample.}
    \label{fig:review_sample}
\end{figure}

Each review text is already tokenized, therefore we only need to convert the dataset into a representation we can work with. We chose to transform the dataset into a Pandas DataFrame with the function defined in listing \ref{lst:load_data}.

\begin{lstlisting}[label={lst:load_data}, caption=Transformation of the dataset.]
def trainset_to_df(path):
    """
    trainset_to_df converts trainset.txt to a pandas dataframe
    Args:
        path: path string to trainset.txt
    Return:
        df: pandas dataframe of trainset.txt
    """

    with open(path, 'r') as f:
        lines = f.readlines()

    lines = [[line.split()[0], line.split()[1], line.split()[2], ' '.join(line.split()[3:])] for line in lines]
    df = pd.DataFrame(lines, columns=['topic', 'sentiment', 'id', 'text'])

    return df
\end{lstlisting}

The function in \ref{lst:load_data} removes any newline or trailing white characters and divides the sample into specific columns: labels, sentiment, id and text.

\subsection{Visualization of data}%Alex

Once we have a data frame to work with we can begin to explore and visualise the data-set. Our goal is to classify the reviews by topic or by sentiment. Thus we need to check if there is any unbalance between those categories.
To do so we can call the value\_counts() method from pandas. and plot the result in a pie.
\begin{lstlisting}
reviews['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%' 
)
\end{lstlisting}
We can see in figure \ref{fig:sentimentdistribution} that there is no unbalance at all between negative and positive reviews. Here negative are at 50.5\% and positive at 49.5\%. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{sentimentdistribution.png}
    \caption{Distribution of sentiment.}
    \label{fig:sentimentdistribution}
\end{figure}

Next we want to explore the distribution of topics between our reviews. Same as for our sentiment vale\_counts() method comes in handy but this time we will plot in a bar plot and we need to set the y axis limit to 960 to see better results, because the difference is very small. 
\begin{lstlisting}
reviews['topic'].value_counts().plot(kind='bar', ylim=960)
\end{lstlisting}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{topicdistribution.png}
    \caption{Distribution of topic.}
    \label{fig:topicdistribution}
\end{figure}

We can see in figure \ref{fig:topicdistribution} that the topic are well distributed. There is not one topic that has a significant difference in amount of reviews compared to the others. We can see thought, that music and dvd have more reviews than health and camera which can indicated that those product are more used then the others. 

Next we want to see if the sentiment are well distributed among the topics. This will show us if there is an unbalance in that category to be aware of.
Here the value\_counts() is not enough. We need the goupby() method of pandas combined with the count()and unstack() method.
\begin{lstlisting}
reviews.groupby(['topic', 'sentiment']).sentiment.count().unstack().plot(kind='bar', ylim=400)
\end{lstlisting}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{sentimentpertopic.png}
    \caption{Distribution of sentiment per topic.}
    \label{fig:sentimentpertopic}
\end{figure}
Figure \ref{fig:sentimentpertopic} indicates that the distribution of sentiment per topic is balanced. We can see that software and camera have the most balanced distribution.

As for curiosity we wanted to explore the text length of our reviews to see if there were any observation we could discover. First we will check the text length of each review and see if we can observe anything.
\begin{lstlisting}
 lengths_of_reviews = [len(review) for review in reviews['text']]
 plt.boxplot(lengths_of_reviews)
 plt.show()
\end{lstlisting}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{textlengthdistribution.png}
    \caption{text length of our reviews}
    \label{fig:textlengthdistribution}
\end{figure}
From figure \ref{fig:textlengthdistribution} we can see that 4 reviews stand out with a large number for its text length. We decided to leave those reviews, because we thought it would not make a difference in the training of the model.\newline
One last observation we wanted to do with our data-set, is to create a word-cloud with the text of our reviews. It could give us insight in what is important in that text for the pre-processing part of this project.

To do so we use WordCloud, a class offered by the worldcloud library. We create our word-cloud with some parameters and generate the words to show with the filtered text of the reviews and divided them by sentiment, to have a word-cloud by sentiment. Moreover  we decided to not include stopwords in this example. Later We will see that it can have an slight effect on the performance of the model.
\begin{lstlisting}
wordcloud = WordCloud(width=1000, height=1000, margin=0).generate(negative_words_withoutStopwords)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()
\end{lstlisting}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{negativewordcloud.png}
    \caption{negative word-cloud.}
    \label{fig:negativewordcloud}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{positivewordcloud.png}
    \caption{positive word-cloud.}
    \label{fig:positivewordcloud}
\end{figure}
The negative word-cloud (fig \ref{fig:negativewordcloud}) shows a that the \textit{nt} is more present than in the positive one. The word \textit{great} is more present in the positive word-cloud than in the negative one. 

The word-cloud made us debate if taking the stop word out was a good idea or not, same for the punctuation.
Overall we could see that the dataset is clean and has no anomalies. We can work with it without having to use Imputers or balancing out the sentiment or topics.  
% In this section, you need to investigate data trend, to explore the data and visualize it. The most important part of understanding the data is identifying the questions that you want to answer and then the second important part is to summarize their main characteristics, often plotting them visually. The plotting in EDA consists of Histograms, Box plot, Scatter plot, and many more. 
% }}}

% Models and approach {{{
\section{Method/Approach/Model}

\subsection{Data pre-processing}

We load our dataset with our \textit{trainset\_to\_df()} function defined in \ref{lst:load_data}:

\begin{lstlisting}
reviews = trainset_to_df('trainset.txt')
\end{lstlisting}

First of all, we check if there are any Nan or missing values inside the dataframe:

\begin{lstlisting}
reviews.isnull().values.any()
\end{lstlisting}

This returns us with \textit{False}, which means we don't have any missing values in the dataset.

As a next step, we verify if any duplicate \textit{id} values exists:

\begin{lstlisting}
reviews['id'].duplicate().any()
# True
reviews['id'].duplicate().value_counts
# True = 5000, False = 1000
\end{lstlisting}

This tells us that we have 5000 duplicate review ids. In order to verify if the duplicate id values and the text values are dependent we sort the dataframe according to the id value as seen in Fig. \ref{fig:id_sort}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{id_sort.png}
    \caption{First samples of reviews sorted by \textit{id}.}
    \label{fig:id_sort}
\end{figure}

There is no relation between the id and text columns, therefore we can remove the id column since it won't be useful as a feature:

\begin{lstlisting}
reviews = reviews[['topic', 'sentiment', 'text']]
\end{lstlisting}

When checking for duplicates in the text column:

\begin{lstlisting}
reviews[reviews['text'].duplicated()].sort_values('text')
\end{lstlisting}

We did not find any duplicates in this column.

After dealing with missing and duplicate values, we continue with binarizing and label encode our variables. First we start to binarize our categorical varible in the sentiment column:

\begin{lstlisting}
reviews['sentiment'] = reviews['sentiment'].apply(lambda sentiment: 0 if sentiment == 'neg' else 1)
\end{lstlisting}

This operation applies the given lambda along the column where it maps \textit{'neg'} string to $0$ and \textit{'pos'} to $1$.

Then we label encode the topic column with \textit{LabelEncoder()} from the \textit{sklearn.preprocessing} library:

\begin{lstlisting}
from sklearn import preprocessing

le = preprocessing.LabelEncoder()
review['topic'] = le.transform(reviews['topic'])

\end{lstlisting}

\subsection{Text Representation}

\textcolor{red}{Santatra}

\subsection{Feature Selection}

\textcolor{red}{Santatra}

\subsection{Implementation}

We compile every preprocessing and feature engineering steps into a function called preprocess\_dataset. This function executes every previously mentioned steps such as removing stopwords from the reviews, binarizing and encoding the respective labels. Additionally, it vectorizes the features with the TFID representation and uses the chi\_square\_test to select features.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{data_preprocessing_function.png}
    \caption{Definition of \textit{preprocess\_dataset()} function.}
    \label{fig:preprocess_dataset}
\end{figure}

\subsection{Performance Measures}

\paragraph{Classification predictions.} In binary classification, an estimator can two types of predictions: \textit{True} or \textit{False}. True predictions can be either a true positive or a true negative where a model correctly predicts the positive or negative class respectively. The same works for the false positive and false negative predictions where the model makes incorrect predictions.

\paragraph{Conclusion matrix.} An error matrix which visualizes the performance of an algorithm. Each row of the matrix represents the samples of an actual class, whereas the columns represent the samples in a predicted class. Fig. \ref{fig:confusion_matrix} visualizes an example of a confusion matrix.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{confusion_matrix.jpg}
    \caption{An example of a confusion matrix.}
    \label{fig:confusion_matrix}
\end{figure}

\paragraph{Accuracy.} A metrix to evaluation the classification of a model. It is defined by the ratio of correct predictions made by the model.

\begin{equation}
    Accuracy = \frac{Number \ of \ correct \ predictions}{Total \ number \ of \ predictions}
\end{equation}

\paragraph{Precision.} Represents the ratio of positive classifications which are actually correct amon the samples classified as positive.

\begin{equation}
    Precision = \frac{TP}{TP + FP}
\end{equation}

\paragraph{Recall.} Represents the ratio of actual positives which were identified correctly among the total number of positive examples.

\begin{equation}
    Recall = \frac{TP}{TP + FN}
\end{equation}

\paragraph{F-score.} This metric combines the precision and recall and is defined as a harmonic mean of these two measures.

\begin{equation}
    F_1 = \frac{TP}{TP + \frac{1}{2}(FP + FN)}
\end{equation}

\paragraph{Cross-validation.} Another metric to assess the performance of a classifier is the cross-validation approach. It assesses how a classifier generalizes on an independent data set. With this approach, we are able to detect overfitting of the model and mitigate biased slection of the training and testing subsets. This method has the following process:

\begin{itemize}
    \item Split the dataset into $k$ subsets
    \item Rotate the training and validation subsets to create $k$ independent trained classifiers
    \item Take the mean of the scores of these $k$ classifiers to assess the overall accuracy performance
\end{itemize}

A visualization of K-Fold, an example of cross-validation, is given is Fig. \ref{fig:cross_validation}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{cross_validation.png}
    \caption{K-fold example where $k = 5$.}
    \label{fig:cross_validation}
\end{figure}

\subsection{Baseline: Training of a model}

% For this section, we need to have a clear overview of your method so try to be precise and explain it in a short form that someone else can understand it easily. 
% If you want to use any math for this section, there is a software that if you give it a picture of your math, you will get the latex code for that math :). The name of the software is \textbf{Mathpix snip}.


\subsubsection{Baseline: Topic classifiction} % Alex
Here we will try to create a model, also called classifier, that will classify reviews into specific topics. 
To train a model we need our data processed and split into a training and testing set. In the splitting process we create 4 variables. 
\begin{itemize}
    \item X\_train is the data used to classify the reviews during training process. In this case it is the text of our reviews. 
    \item y\_train is the label used to tell the model if he classified correctly or not during training process. In this case the topics of our reviews.
    \item X\_test is the data used to classify the reviews during testing process.
    \item y\_test is the label used to tell the model if he classified correctly or not during testing process.
\end{itemize} 

The splitting is handled with the train\_test\_split() method from sklearn framework. We give this method as parameters, the data, the labels and the percentage size the testing part should have.
\begin{lstlisting}
X_train, X_test, y_train, y_test = train_test_split(processedText, labels, test_size=0.2, random_state=0)
\end{lstlisting}

Once we have our training and testing variables we need to create the model that will be trained and classify our data.
Sklearn gives us many option to chose from and we will try 4 of them with default hyper parameters. Once we found good options for our model we can try different hyper parameters to optimise the classification.

\begin{lstlisting}
model = MultinomialNB()
model = RandomForestClassifier()
model = SVC()
model = KNeighborsClassifier()
\end{lstlisting}
From there we can comment out all but one model and train it with our data. Using fit() method and giving X\_train and y\_train as parameters the model will train using those variables and learn until it went through the whole set.
\begin{lstlisting}
model.fit(X_train, y_train)
\end{lstlisting}
Once the training is done we want to know how efficient our model is at classifying our reviews. We use our testing variable for this task and the predict() method, to save the prediction in a new variable.
\begin{lstlisting}
predictions = text_classifier.predict(X_test)
\end{lstlisting}
With the prediction saved in a variable we can now use them and compare them to the correct labels and calculate the different performance measures. 
Here three methods are very useful:
\begin{lstlisting}
cm = confusion_matrix(y_test, predictions)
cr = classification_report(y_test,predictions)
accuracy = accuracy_score(y_test, predictions)
\end{lstlisting}
The accuracy can also be found in the classification report but, accuracy\_score() is more precise. We get from these three methods all the performance measures we need, explained in section 4.5.
From trying out all the models Multinomial Naive Bayes and SVC are our best guesses to work with. But in later section we will present better option to find the most optimised model.
\subsubsection{Baseline: Sentiment classifiction}

\textcolor{red}{Santatra}

\subsubsection{Performance evaluation with punctuation}

\paragraph{Sentiment classifiction.}

In this section, we show case the test accuracies for the sentiment classifiction. In Fig. \ref{tab:sentiment_without_punctuation} we trained the models without including punctuations, whereas in Fig. \ref{tab:sentiment_with_punctuation} we included them during the training.

\begin{table}[!ht]
\centering
\caption{Sentiment classifiction test accuracy scores without punctuation.}
\label{tab:sentiment_without_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.822 \\ \hline
MultinomialNB & 0.807 \\ \hline
RandomForestClassifier & 0.800 \\ \hline
KNeighborsClassifier & 0.743 \\ \hline
DecisionTreeClassifier & 0.687 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Sentiment classifiction test accuracy scores with punctuation.}
\label{tab:sentiment_with_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.825 \\ \hline
MultinomialNB & 0.813 \\ \hline
RandomForestClassifier & 0.804 \\ \hline
KNeighborsClassifier & 0.732 \\ \hline
DecisionTreeClassifier & 0.663 \\ \hline
\end{tabular}
%
}
\end{table}

\paragraph{Topic classifiction.}

In this section, we show case the test accuracies for the topic classifiction. In Fig. \ref{tab:topic_without_punctuation} we trained the models without including punctuations, whereas in Fig. \ref{tab:topic_with_punctuation} we included them during the training.

\begin{table}[ht]
\centering
\caption{Topic classifiction test accuracy scores without punctuation.}
\label{tab:topic_without_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.919 \\ \hline
MultinomialNB & 0.916 \\ \hline
KNeighborsClassifier & 0.896 \\ \hline
RandomForestClassifier & 0.880 \\ \hline
DecisionTreeClassifier & 0.798 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[ht]
\centering
\caption{Topic classifiction test accuracy scores with punctuation.}
\label{tab:topic_with_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
MultinomialNB & 0.923 \\ \hline
SVC	& 0.919 \\ \hline
KNeighborsClassifier & 0.889 \\ \hline
RandomForestClassifier & 0.885 \\ \hline
DecisionTreeClassifier & 0.797 \\ \hline
\end{tabular}
%
}
\end{table}

During this analysis, we observed that the overall performance of the models improve when including the punctuation in the review text when training the models.

\subsection{Ensemble methods}

\textcolor{red}{Santatra}

\subsection{Grid Search}

Grid search is an approach to find the optimal hyperparameters of a model which yields the most accurate predictions. Hyperparameter is a characteristics of an estimator which cannot be learned from data. These hyperparameters have to be set before training. During grid search we try to find the best hyperparameters values through an exhaustive search over specified parameter values for an estimator.

For this study, we implemented a grid search framework which iterates over possible estimator candidates and executes grid search for each classifer while considering the defined hyperparameter search space.

\subsubsection{Grid Search: Base classifiers}

We executed grid search for both sentiment and topic classification tasks. The best hyperparameters for the sentiment and topic classification task can be consulted in Table \ref{tab:sentiment_gs} and in Table \ref{tab:topic_gs} respectively. The \textit{best\_score} in these tables represent the training accuracies.

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the sentiment classifiction task.}
\label{tab:sentiment_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_params & best\_score & std\_test\_score \\ \hline
SVC	& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\}	& 0.824	& 0.022 \\ \hline
MultinomialNB & \{'alpha': 10\}	& 0.807	& 0.031 \\ \hline
RandomForestClassifier & \{'class\_weight': None, 'criterion': 'entropy', 'n\_estimators': 70\}	& 0.802	& 0.014 \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 85, 'p': 2\}	& 0.742	& 0.017 \\ \hline
DecisionTreeClassifier & \{'class\_weight': 'balanced', 'criterion': 'entropy', 'splitter': 'best'\}	& 0.692	& 0.024 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the topic classifiction task.}
\label{tab:topic_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_params & best\_score & std\_test\_score \\ \hline
SVC	& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\}	& 0.919	& 0.003 \\ \hline
MultinomialNB & \{'alpha': 1\}	& 0.917	& 0.011 \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 95, 'p': 2\}	& 0.890	& 0.008 \\ \hline
RandomForestClassifier & \{'class\_weight': 'balanced\_subsample', 'criterion': 'gini', 'n\_estimators': 90\}	& 0.881	& 0.008 \\ \hline
DecisionTreeClassifier & \{'class\_weight': None, 'criterion': 'gini', 'splitter': 'random'\}	& 0.800	& 0.023 \\ \hline
\end{tabular}
%
}
\end{table}

\subsubsection{Grid Search: Ensemble methods}

\textcolor{red}{Santatra}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the sentiment classifiction task.}
\label{tab:sentiment_em_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_params & best\_score & std\_test\_score \\ \hline
SVC	& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\}	& 0.824	& 0.022 \\ \hline
MultinomialNB & \{'alpha': 10\}	& 0.807	& 0.031 \\ \hline
RandomForestClassifier & \{'class\_weight': 'None', 'criterion': 'entropy', 'n\_estimators': 70\}	& 0.802	& 0.014 \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 85, 'p': 2\}	& 0.742	& 0.017 \\ \hline
DecisionTreeClassifier & \{'class\_weight': 'balanced', 'criterion': 'entropy', 'splitter': 'best'\}	& 0.692	& 0.024 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the topic classifiction task.}
\label{tab:topic_em_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_params & best\_score & std\_test\_score \\ \hline
SVC	& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\}	& 0.822	& 0.015 \\ \hline
MultinomialNB & \{'alpha': 1\}	& 0.807	& 0.013 \\ \hline
RandomForestClassifier & \{'class\_weight': 'balanced\_subsample', 'criterion': 'gini', 'n\_estimators': 70\}	& 0.800	& 0.005 \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 95, 'p': 2\}	& 0.743	& 0.01 \\ \hline
DecisionTreeClassifier & \{'class\_weight': None, 'criterion': 'entropy', 'splitter': 'random'\}	& 0.687	& 0.005 \\ \hline
\end{tabular}
%
}
\end{table}

% }}}

% Results and discussion {{{
\section{Results \& Discussion}%Alex

%This is an important section in your report. Try to spend more time on this section.
\subsection{Results}


\begin{table}[!ht]
\centering
\caption{Sentiment classification final assessment\label{tab:sentimentassessment}}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|l|}
\hline
             & best\_score\\ \hline
Stacking Classifier     & 0.833     \\ \hline
SVC                     & 0.825     \\ \hline
Extra Trees Classifier  & 0.814     \\ \hline
MultinomialNB           & 0.813     \\ \hline
RandomForestClassifier  & 0.805     \\ \hline
Bagging Classifier      & 0.801     \\ \hline
XGBoost                 & 0.788     \\ \hline
Gradient Boosting       & 0.787     \\ \hline
KNeighborsClassifier    & 0.732     \\ \hline
DecisionTreeClassifier  & 0.663     \\ \hline

\end{tabular}}
\end{table}

\begin{table}[!ht]
\centering
\caption{Topic classification final assessment\label{tab:topicassessment}}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|l|}
\hline
             & best\_score\\ \hline
Stacking Classifier     & 0.929     \\ \hline
MultinomialNB           & 0.923     \\ \hline
SVC                     & 0.919     \\ \hline
Bagging Classifier      & 0.907     \\ \hline
Extra Trees Classifier  & 0.893     \\ \hline
KNeighborsClassifier    & 0.889     \\ \hline
XGBoost                 & 0.886     \\ \hline
RandomForestClassifier  & 0.885     \\ \hline
Gradient Boosting       & 0.865     \\ \hline
DecisionTreeClassifier  & 0.797     \\ \hline

\end{tabular}}
\end{table}
We can see on table \ref{tab:topicassessment} and \ref{tab:sentimentassessment} in order the best scores for each model we have tested with the grid search. The result for both topic and sentiment classification are highest with the Stacking Classifier. These best\_scores are being extracted from the Grid Search explained before and grouped in one table with the results of the base classifiers and the ensemble methods. These results use the data set pre-processed, but doesn't make use of cross-validation. It mainly helped to find the best model to use for classifying reviews. Once the model is selected we can train it using cross-validation and enhance slightly its performance. As said before the best\_score here represents the training accuracy and might be slightly higher than the final testing accuracy. You can find the hyper-parameters and std\_score of the models from table \ref{tab:topicassessment} and \ref{tab:sentimentassessment} in table \ref{tab:topic_gs} and \ref{tab:topic_em_gs} or \ref{tab:sentiment_gs} and \ref{tab:sentiment_em_gs} respectively. Now that we have the results of our research we can discuss on how would be the perfect model for classifying reviews according to sentiment or topic. 

% You can split \verb!Results! and \verb!Discussion! sections. Use the possibility of adding table \ref{tab:sample} and figure \ref{fig:boxplot} to make your report to be easier to follow for readers! 
% You can use following websites to make your latex table:

% \begin{itemize}
%     \item {\href{https://www.latex-tables.com/}{The first suggestion}}
%     \item {\href{https://www.tablesgenerator.com/}{The second suggestion}}
% \end{itemize}

% Figure \ref{fig:boxplot} is a sample of a figure which you can use in your report. 

% \begin{figure}[!hbtp]
%   \centering
%   \includegraphics[width = 1\linewidth, height = 0.5\linewidth]{boxplot_age_ADS.png}
%   \caption{\scriptsize Box plot of participants age.\label{fig:boxplot}}
% \end{figure}

\subsection{Discussion}
With the results we clearly see that Stacking Classifier would be the best option in terms of Classifier for this job. Unfortunately it has the disadvantage to require a lot of computing power. With a normal computer it would take numerous minutes to train this kind of model and test it. In case of critical works like health classification such decision would be important, but for this kind of operation SVC or MultinomialNB is probably more adapted. Classifying reviews is not as critical , thus using a lighter model in terms of computing power would be preferable. Moreover the difference here is only of +/- 0.010. \newline
Another point of discussion is the use of punctuation in data-sets like these. We can clearly imagine a upset customer leaving a review with multiple !, to show his disappointment. Therefore we decided to leave the punctuation in the data-set and not take them away during pre-processing.\newline
Overall the grid search method used during this project has offered a lot of help in finding the right model and the most suited hyper-parameters. It requires some computing power but it saves a lot of time due to the fact that we can let it run and it executes all the possibilities for us. \newline
We can imagine the final step of a project like this one as such: 
\begin{itemize}
    \item Create an SVC model with hyper-parameters taken from grid search.
    \item Use our processed data and start training the model using cross-validation.
    \item use the trained model in an back-end application to classify reviews according to their sentiment or topic, depending what you trained our model for.
\end{itemize}

% }}}

% Conclusion {{{
\section{Conclusion}

% Summary your findings in one or two paragraphs. If you have any reference to support your work, you can also add them to your report \cite{gerven1997comparative}. 
% This is a template for your report, please submit the pdf file for your report at the end. 
% The name of pdf file should be:

We presented our work on sentiment analysis for the \textit{text} project from phase 2 of the DSH course. During this project, we try to classifiy product reviews to a sentiment or a review topic. We achieved for sentiment and topic classification test accuracies of 0.833\% and 0.929\% respectively.

These test accuracies were obtained by the stacking classifiers. We discussed which type of classifiers should be selected and concluded that it depends on the setting in which it will be used.
% }}}

% Table of collaborations {{{
\section{Table of collaborations}

You will find the task descriptions executed by each group member in Table \ref{tab:collab} in the Appendix.

% In this section, we want you briefly describe what each one did in your group for the assigned project (Table \ref{tab:collab}).
% }}}

% Bibliography {{{
\bibliographystyle{apalike}
\raggedright
{\scriptsize
\bibliography{ref.bib}}
% }}}

% Appendix {{{
\onecolumn
\newpage
\clearpage
\section{Appendix}
% }}}

% Task descriptions {{{
\begin{table}[!h]
\centering
\caption{Task descriptions executed by each student of the group.}\label{tab:collab}
\begin{tabular}{|c| >{\centering\arraybackslash}m{0.6\textwidth}|}\hline

\textbf{Group member}  & \textbf{Task} \\

\hline

\multirow{2}{*}{\textbf{Leminh Nguyen}}

                  & Helping out with implementing the data visualisation \\\cline{2-2}
                  & Implementation of dataset conversion to Pandas DataFrame representation \\\cline{2-2}
                  & Implementation of data pre-processing (Data cleaning, selection, normalization) \\\cline{2-2}
                  & Define performance measure + cross validation \\\cline{2-2}
                  & Definition of Grid Search and implementation of grid search framework to iterate over a set of classifer-hyperparameters candidates \\\cline{2-2}
                  & Performance evaluation with punctuation \\\cline{2-2}
                  & Performance evaluation without punctuation \\\cline{2-2}
                  & Sentiment Grid search benchmark \\\cline{2-2}
                  & Topic Grid search benchmark \\\cline{2-2}
                  % & Final benchmark assessment \\\cline{2-2}
                  % & Best model selection \\\cline{2-2}
                  % & Formulate abstract, introduction, discussion and conclusion sections \\\cline{2-2}
                  & Formulate abstract, introduction conclusion sections \\\cline{2-2}
                  & Format the report template \\

\hline

\multirow{2}{*}{\textbf{Alex Poldrugo}}

                  & Implementing the data visualisation.\\\cline{2-2}
                  & Perform the review to topic classification.\\\cline{2-2}
                  & Include KNN model to our research.\\\cline{2-2}
                  & Formulate Baseline model, NB and SVC model explanation and visualisation of data section.\\\cline{2-2}
                  & Formulate Result and Discussion section.\\\cline{2-2}
                  & Code comparison.\\

\hline

\multirow{2}{*}{\textbf{Rafidison Rakotondrasoa}}

                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\\cline{2-2}
                  & Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur quam arcu, scelerisque id fringilla ut, finibus ut ligula. Aliquam sagittis.  \\

\hline

\end{tabular}
\end{table}
% }}}

% Comments {{{
% \section{Pdf}
% Please save your file as a pdf file and apply the following template for naming your pdf file.
% \textit{DSH\_nameofyourproject\_phasenumber}
% \noindent Good luck :)
% }}}

\end{document}
