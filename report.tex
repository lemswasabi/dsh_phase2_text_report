% packages {{{
% File lfd1617.tex
%
%% Based on the style files for EACL-2017
%% Based on the style files for ACL-2016
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks]{hyperref}

\usepackage{pdflscape}
\hypersetup{
    colorlinks,
    linkcolor={red},
    linktoc=page,
    citecolor={blue},
    urlcolor={blue}
}

\usepackage{caption}
\captionsetup[table]{font={stretch=1.2}}
\captionsetup[figure]{font={stretch=1.2}}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle, language=Python}

%%%% LEAVE THIS IN
\eaclfinalcopy

\graphicspath{{./figures/}}

\newcommand\BibTeX{B{\sc ib}\TeX}% }}}
  
% {{{ title
\title{DSH - Sentiment Analysis}
%Add the authors name alphabetically!
\author{Leminh Nguyen
\textsuperscript{1}, Alex Poldrugo\textsuperscript{2}, Rafidison Santatra Rakotondrasoa\textsuperscript{3},\\
%\vspace{0pt}
\\
\textsuperscript{1}0180531722,  \textsuperscript{2} 015063762f \textsuperscript{3} 0190252530\\
\texttt{\{le.nguyen.001, alex.poldrugo.001, \& rafidison.rakotondrasoa.001\}@student.uni.lu} \\ %in case your adding Lu email address use this template and if you want to add different email address from your coauthor use the following template. 
% \texttt{nnn.xzx23@yahoo.com}\\
% \texttt{xxx.yyy@yahoo.com}
}
\date{\date}  

\pagenumbering{roman}% }}}

\begin{document}

\maketitle

% abstract {{{
\begin{abstract}
    \textbf{During this project, we classify product reviews according to sentiments and the topic of the reviews. Investigating the given dataset, each product review sample contains the text of a review written by a customer annotated with their sentiment and topic label. These texts were already tokenized in the given dataset. Our objective is to apply exploratory data analysis (EDA) to investigate the data trend, explore and visualize the dataset. After exploring our data variables and extracting features from the samples, we set a baseline that consists of a set of machine learning models such as the Naive Bayes and Random Forest classifiers. In another iteration, we investigated the ensemble methods estimator and fine-tuned the hyperparameters of all our classifier candidates. Furthermore, with the best-tuned models, we achieved for the sentiment and topic classification testing accuracies of 0.833\% and 0.929\% respectively. Finally, we discuss the selection of the models that can be used in production and concluded that it depends on the setting in which it will be used.}
\end{abstract}
% }}}

% Introduction {{{
\section{Introduction}

% Problem statement

In this study, we are working with a dataset of reviews. Our goal is to classify the review samples according to their sentiment, which can be \textit{negative} or \textit{positive}. Additionally, we classify these samples to the topic of the reviews: \textit{books}, \textit{camera}, \textit{dvd}, \textit{health}, \textit{music} or \textit{software}.\newline
In order to do so we experimented with different types of model provided by the scikit-learn library. We tried to find the best model that can classify our reviews into the said categories. To find the best model we used grid search which makes an exhaustive search with given parameters to find the model with the best accuracy.\newline
In this project we go through all the steps and explain how we explore the data set, find the best model and how to train a model. 
% We use the modules in scikit-learn to split data into training and testing sets. And we train the data and estimate how well it is likely to perform on out-of-sample data through evaluation functions such as precision, recall, f-score, and also with confusion matrix. And the distinct values of prior and posterior probabilities also shows how important is the additional information taken into account. 
% }}}

% Data {{{
\section{Data}%Alex
Our data represents reviews for certain products. The reviews have been categorised by sentiment and topic. The sentiment is divided into two values, negative and positive. The topic is divided into 6 different types of products, books, camera, dvd, health, music, software. The data has also an id for each review, which represents the name of the text file the review comes from. And finally, we have the text category, which is the textual comment part of the review.\newline
We have in total 6000 reviews in our dataset. Each row of our data represents a review and the column represent each topic, sentiment, id and text.  

%In this section, you need to describe your dataset in details. 
% }}}

% EDA {{{
\section{Exploratory Data Analysis (EDA)}

In this section, we investigate data trend, to explore the data and visualize it.

\subsection{Loading of data}

Before we can explore the dataset and visualise it, we first need to transform the dataset into a representation that we can work with. The given dataset is stored in a \textit{.txt} file. Each row of this file contains a review sample which is delimited by a newline character. Each sample is divided into sections. The first word represents the topic of the review, whereas the second word depicts its sentiment. The third word of the sample serves as id. The rest of the row presents the review text itself. This is visualised in Fig. \ref{fig:review_sample}, in which we can see one of the many rows of the dataset. The first word / token represents which topic the review belongs, second one is the sentiment, third one is the id, that determines from which txt file the review has been extracted and the rest of that row is the text of the review.

% In Fig. \ref{fig:dataset_txt}, we visualized the structure of the .txt file.
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{dataset_txt.png}
%     \caption{\scriptsize Preview of the .txt file containing the review samples.}
%     \label{fig:dataset_txt}
% \end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{review_sample.png}
    % \caption{\scriptsize Layout of a review sample.}
    \caption{\scriptsize Extract from a row of the original dataset plus description.}
    \label{fig:review_sample}
\end{figure}

Each review text is already tokenized, therefore we only need to convert the dataset into a representation we can work with. We chose to transform the dataset into a Pandas DataFrame with the function defined in listing \ref{lst:load_data}.

\begin{lstlisting}[label={lst:load_data}, caption=Transformation of the dataset.]
def trainset_to_df(path):
    """
    trainset_to_df converts trainset.txt to a pandas dataframe
    Args:
        path: path string to trainset.txt
    Return:
        df: pandas dataframe of trainset.txt
    """

    with open(path, 'r') as f:
        lines = f.readlines()

    lines = [[line.split()[0], line.split()[1], line.split()[2], ' '.join(line.split()[3:])] for line in lines]
    df = pd.DataFrame(lines, columns=['topic', 'sentiment', 'id', 'text'])

    return df
\end{lstlisting}

The function in \ref{lst:load_data} removes any newline or trailing white characters and divides the sample into specific columns: labels, sentiment, id and text.

\subsection{Visualization of data}%Alex

Once we have a data frame to work with we can begin to explore and visualise the data set. Our goal is to classify the reviews by topic or by sentiment. Thus we need to check if there is an imbalance between those categories.
To do so we can call the value\_counts() method from pandas. and plot the result in a pie.
\begin{lstlisting}
reviews['sentiment'].value_counts().plot(kind='pie', autopct='%1.1f%%' 
)
\end{lstlisting}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{sentimentdistribution.png}
    \caption{\scriptsize Percentage distribution of positive and negative reviews across the dataset}
    \label{fig:sentimentdistribution}
\end{figure}
Figure \ref{fig:sentimentdistribution} shows the distribution of sentiment across our datset. Here the orange part represent the percentage of positive reviews in our dataset and blue the negative once. We see that there is no unbalance at all between negative and positive reviews. Here negative are at 50.5\% and positive at 49.5\%. 


Next we want to explore the distribution of topics between our reviews. Same as for our sentiment vale\_counts() method comes in handy but this time we will plot in a bar plot and we need to set the y axis limit to 960 to see better results, because the difference is very small. 
\begin{lstlisting}
reviews['topic'].value_counts().plot(kind='bar', ylim=960)
\end{lstlisting}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{topicdistribution.png}
    \caption{\scriptsize Number of reviews for each topic across the dataset.}
    \label{fig:topicdistribution}
\end{figure}

Figure \ref{fig:topicdistribution} shows the distribution of topics across our reviews. Each bar represents the number of reviews belonging to the topic specified under it. The topics are well distributed. There is not one topic that has a significant difference in amount of reviews compared to the others. We can see thought, that music and dvd have more reviews than health and camera which can indicated that those product are more used then the others. 

Next we want to see if the sentiment are well distributed among the topics. This will show us if there is an unbalance in that category to be aware of.
Here the value\_counts() is not enough. We need the goupby() method of pandas combined with the count()and unstack() method.
\begin{lstlisting}
reviews.groupby(['topic', 'sentiment']).sentiment.count().unstack().plot(kind='bar', ylim=400)
\end{lstlisting}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{sentimentpertopic.png}
    \caption{\scriptsize Number of negative and positive reviews for each topic.}
    \label{fig:sentimentpertopic}
\end{figure}
Figure \ref{fig:sentimentpertopic} indicates the distribution of sentiment per topic. Each orange bar shows the amount of positive reviews that belong to the specific topic and the blue bar shows the same but for negative reviews.The distribution of sentiment per topic is balanced as well. We can see that software and camera have the most balanced distribution.

As for curiosity we wanted to explore the text length of our reviews to see if there were any observation we could discover. First we will check the text length of each review and see if we can observe anything.

\begin{lstlisting}
 lengths_of_reviews = [len(review) for review in reviews['text']]
 plt.boxplot(lengths_of_reviews)
 plt.show()
\end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{textlengthdistribution.png}
    \caption{\scriptsize Representation of number of characters in the text part of the review for each review.}
    \label{fig:textlengthdistribution}
\end{figure}

Figure \ref{fig:textlengthdistribution} shows the text length for each review. Each small circle we can see represents one review and its position on the y axis represent the number of characters in that review. 4 reviews stand out with a large number of characters. We decided to drop those reviews, because we thought it would not make a difference in the training of the model.\newline

One last observation we wanted to do with our data-set, is to create a word-cloud with the text of our reviews. It could give us insight in what is important in that text for the pre-processing part of this project.

To do so we use WordCloud, a class offered by the worldcloud library. We create our word-cloud with some parameters and generate the words to show with the filtered text of the reviews and divided them by sentiment, to have a word-cloud by sentiment. Moreover  we decided to not include stopwords in this example. Later We will see that it can have an slight effect on the performance of the model.
\begin{lstlisting}
wordcloud = WordCloud(width=1000, height=1000, margin=0).generate(negative_words_withoutStopwords)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()
\end{lstlisting}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{negativewordcloud.png}
    \caption{\scriptsize Word-cloud of the text from negative reviews.}
    \label{fig:negativewordcloud}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{positivewordcloud.png}
    \caption{\scriptsize Word-cloud of the text from positive reviews.}
    \label{fig:positivewordcloud}
\end{figure}
The negative word-cloud (fig \ref{fig:negativewordcloud}) shows a that the \textit{nt} is more present than in the positive one (fig \ref{fig:positivewordcloud}). The word \textit{great} is more present in the positive word-cloud than in the negative one. 

The word-cloud made us debate if taking the stop word out was a good idea or not, same for the punctuation.
Overall we could see that the dataset is clean and has no anomalies. We can work with it without having to use Imputers or balancing out the sentiment or topics.  
% In this section, you need to investigate data trend, to explore the data and visualize it. The most important part of understanding the data is identifying the questions that you want to answer and then the second important part is to summarize their main characteristics, often plotting them visually. The plotting in EDA consists of Histograms, Box plot, Scatter plot, and many more. 
% }}}

% Models and approach {{{
\section{Method/Approach/Model}

\subsection{Data pre-processing}

We load our dataset with our \textit{trainset\_to\_df()} function defined in Listing \ref{lst:load_data}:

\begin{lstlisting}
reviews = trainset_to_df('trainset.txt')
\end{lstlisting}

First of all, we check if there are any Nan or missing values inside the dataframe:

\begin{lstlisting}
reviews.isnull().values.any()
\end{lstlisting}

This returns us with \textit{False}, which means we don't have any missing values in the dataset.

As a next step, we verify if any duplicate \textit{id} values exists:

\begin{lstlisting}
reviews['id'].duplicate().any()
# True
reviews['id'].duplicate().value_counts
# True = 5000, False = 1000
\end{lstlisting}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{id_sort.png}
    \caption{\scriptsize First samples of reviews sorted by \textit{id}.}
    \label{fig:id_sort}
\end{figure}
This tells us that we have 5000 duplicate review ids. To verify if the duplicate id values and the text values are dependent we sort the dataframe according to the id value as seen in Fig. \ref{fig:id_sort}.



There is no relation between the id and text columns, therefore we can remove the id column since it won't be useful as a feature:

\begin{lstlisting}
reviews = reviews[['topic', 'sentiment', 'text']]
\end{lstlisting}

When checking for duplicates in the text column:

\begin{lstlisting}
reviews[reviews['text'].duplicated()].sort_values('text')
\end{lstlisting}

We did not find any duplicates in this column.

After dealing with missing and duplicate values, we continue with binarizing and label encode our variables. First we start to binarize our categorical variable in the sentiment column:

\begin{lstlisting}
reviews['sentiment'] = reviews['sentiment'].apply(lambda sentiment: 0 if sentiment == 'neg' else 1)
\end{lstlisting}

This operation applies the given lambda along the column where it maps \textit{'neg'} string to $0$ and \textit{'pos'} to $1$.

Then we label encode the topic column with \textit{LabelEncoder()} from the \textit{sklearn.preprocessing} library:

\begin{lstlisting}
from sklearn import preprocessing

le = preprocessing.LabelEncoder()
review['topic'] = le.transform(reviews['topic'])

\end{lstlisting}

\subsection{Text Representation}

Here we want to talk about a lot of the different ways we can represent words to use in machine learning. As we already mentioned before, we want to identify the sentiment that can be found in a text using a learning model. But, we will work with learning model which will not process data in raw form (text), then we need to convert text in the form of numbers, specifically some numeric vectors. We cannot simply give these sentences to a learning model and ask it to tell us whether a text was positive or negative. We need to perform certain text preprocessing steps, this is where Bag Of Words (BOW) and TF-IDF are introduced. We can represent a sentence as a bag of words vector (a string of numbers). The idea is to create vector features from text which will represent the text. We saw two ways to do that: \emph{Count Vectorizer} and \emph{TF-IDF vectorizer}.

\subsubsection{Count Vectorizer}

Count Vectorization is one of the most basic ways we can represent text data numerically. The idea is very simple, we will be creating vectors that have a dimensionality equal to the size of our vocabulary, and if the text data features that word, we will put a one in that dimension. Every time we encounter that word again, we will increase the count, leaving 0s everywhere we did not find the word even once.

% \begin{lstlisting}
% from sklearn.feature_extraction.text import CountVectorizer

% cv = CountVectorizer(max_features = None)
% X = cv.fit_transform(reviews['text']).toarray()
% \end{lstlisting}

Figure \ref{fig:countvect} shows 3 text examples and and how those text vectors after applying Count Vectorizer.

\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm, height=4cm]{1.png}
    \caption{\scriptsize Transformation of text into vectors using Count Vectorizer}
    \label{fig:countvect}
\end{figure}

\subsubsection{TF-IDF Vectorizer}

TF-IDF stands for Term Frequency-Inverse Document Frequency. It wanst to answer the question "how important a word is to a document in a collection or corpus". We want to compute the TF-IDF score for each word in the corpus. Words with a higher score are more important, and those with a lower score are less important. 

% \begin{lstlisting}
% from sklearn.feature_extraction.text import TfidfVectorizer

% cv = TfidfVectorizer(max_features = None)
% X = cv.fit_transform(reviews['text']).toarray()
% \end{lstlisting}

Figure \ref{fig:tfidfvect} shows 3 text examples and and how those text vectors after applying Count Vectorizer.

\begin{figure}[h!]
    \centering
    \includegraphics[width=7cm, height=4cm]{2.png}
    \caption{\scriptsize Transformation of text into vectors using TF-IDF Vectorizer}
    \label{fig:tfidfvect}
\end{figure}

\subsection{Feature Selection}

Feature selection is an interesting process because we want to avoid the features that are not relevant. From the previous section, we got feature vectors as a result to represent the data text, however vectors contain too much zeros and irrelevant data. In this section, we want to select only the important features that will be necessary for the sentiment and topic classification. For that, we saw two kinds of feature classification, which are \emph{removing features with low variance} and \emph{rnivariate feature selection}.
\newpage

\subsubsection{Removing features with low variance}

We use Variance Threshold to do feature selection on our sample vectors. By default, it removes all zero-variance features, i.e. features that have the same value in all samples (partially similar to the TF-IDF features creation).\newline
\begin{figure}[h!]
    \centering
    \includegraphics[width=5cm, height=6cm]{3.png}
    \caption{\scriptsize Changes in the data after removing zero-variance features}
    \label{fig:removfeatures}
\end{figure}

In figure \ref{fig:removfeatures} we can see the number of columns we drop in that process. From 37109 columns after TF-IDF Vectorizer, to 7307 columns after removing the low variance features.
\subsubsection{Univariante feature selection}

We perform a Chi-Square test to the samples to retrieve only the k best features. Here k is 4000.\newline
\begin{figure}[h!]
    \centering
    \includegraphics[width=3.5cm, height=5.5cm]{4.png}
    \caption{\scriptsize Changes in the data after using Univariate feature selection that retrieves the 4000 best features}
    \label{fig:univfeature}
\end{figure}

In figure \ref{fig:univfeature} we can see the number of columns we drop in that process. From 37109 columns after TF-IDF Vectorizer, to 4000 columns after performing Chi-square feature selection.


\subsection{Implementation}

We compile every preprocessing and feature engineering steps into a function called preprocess\_dataset. This function executes every previously mentioned steps such as removing stopwords from the reviews, binarizing and encoding the respective labels. Additionally, it vectorizes the features with the TFID representation and uses the chi\_square\_test to select features.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{data_preprocessing_function.png}
    \caption{\scriptsize Definition of \textit{preprocess\_dataset()} function.}
    \label{fig:preprocess_dataset}
\end{figure}

\subsection{Performance Measures}

\paragraph{Classification predictions.} In binary classification, an estimator can two types of predictions: \textit{True} or \textit{False}. True predictions can be either a true positive or a true negative where a model correctly predicts the positive or negative class respectively. The same works for the false positive and false negative predictions where the model makes incorrect predictions.

\paragraph{Conclusion matrix.} An error matrix that visualizes the performance of an algorithm. Each row of the matrix represents the samples of an actual class, whereas the columns represent the samples in a predicted class. Fig. \ref{fig:confusion_matrix} visualizes an example of a confusion matrix.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{confusion_matrix.jpg}
    \caption{\scriptsize An example of a confusion matrix.}
    \label{fig:confusion_matrix}
\end{figure}

\paragraph{Accuracy.} A metric to evaluate the classification of a model. It is defined by the ratio of correct predictions made by the model.

\begin{equation}
    Accuracy = \frac{Number \ of \ correct \ predictions}{Total \ number \ of \ predictions}
\end{equation}

\paragraph{Precision.} Represents the ratio of positive classifications which are correct among the samples classified as positive.

\begin{equation}
    Precision = \frac{TP}{TP + FP}
\end{equation}

\paragraph{Recall.} Represents the ratio of actual positives which were identified correctly among the total number of positive examples.

\begin{equation}
    Recall = \frac{TP}{TP + FN}
\end{equation}

\paragraph{F-score.} This metric combines the precision and recall and is defined as a harmonic mean of these two measures.

\begin{equation}
    F_1 = \frac{TP}{TP + \frac{1}{2}(FP + FN)}
\end{equation}

\paragraph{Cross-validation.} Another metric to assess the performance of a classifier is the cross-validation approach. It assesses how a classifier generalizes on an independent data set. With this approach, we can detect the overfitting of the model and mitigate a biased selection of the training and testing subsets. This method has the following process:

\begin{itemize}
    \item Split the dataset into $k$ subsets
    \item Rotate the training and validation subsets to create $k$ independent trained classifiers
    \item Take the mean of the scores of these $k$ classifiers to assess the overall accuracy performance
\end{itemize}

There are multiple version of methods for cross-validation. K-Fold is the method we decide to use in our project. A visualization of it is given in Fig. \ref{fig:cross_validation}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{cross_validation.png}
    \caption{\scriptsize cross-validation using K-fold with $k = 5$.}
    \label{fig:cross_validation}
\end{figure}

\subsection{Baseline: Training of a model}

% For this section, we need to have a clear overview of your method so try to be precise and explain it in a short form that someone else can understand it easily. 
% If you want to use any math for this section, there is a software that if you give it a picture of your math, you will get the latex code for that math :). The name of the software is \textbf{Mathpix snip}.


\subsubsection{Baseline: Topic classification} % Alex
Here we will try to create a model, also called classifier, that will classify reviews into specific topics. 
To train a model we need our data processed and split into a training and testing set. In the splitting process we create 4 variables. 
\begin{itemize}
    \item X\_train is the data used to classify the reviews during training process. In this case it is the text of our reviews. 
    \item y\_train is the label used to tell the model if he classified correctly or not during training process. In this case the topics of our reviews.
    \item X\_test is the data used to classify the reviews during testing process.
    \item y\_test is the label used to tell the model if he classified correctly or not during testing process.
\end{itemize} 

The splitting is handled with the train\_test\_split() method from sklearn framework. We give this method as parameters, the data, the labels and the percentage size the testing part should have.
\begin{lstlisting}
X_train, X_test, y_train, y_test = train_test_split(processedText, labels, test_size=0.2, random_state=0)
\end{lstlisting}

Once we have our training and testing variables we need to create the model that will be trained and classify our data.
Sklearn gives us many option to chose from and we will try 4 of them with default hyper parameters. Once we found good options for our model we can try different hyper parameters to optimise the classification.

\begin{lstlisting}
model = MultinomialNB()
model = RandomForestClassifier()
model = SVC()
model = KNeighborsClassifier()
\end{lstlisting}
From there we can comment out all but one model and train it with our data. Using fit() method and giving X\_train and y\_train as parameters the model will train using those variables and learn until it went through the whole set.
\begin{lstlisting}
model.fit(X_train, y_train)
\end{lstlisting}
Once the training is done we want to know how efficient our model is at classifying our reviews. We use our testing variable for this task and the predict() method, to save the prediction in a new variable.
\begin{lstlisting}
predictions = text_classifier.predict(X_test)
\end{lstlisting}
With the prediction saved in a variable we can now use them and compare them to the correct labels and calculate the different performance measures. 
Here three methods are very useful:
\begin{lstlisting}
cm = confusion_matrix(y_test, predictions)
cr = classification_report(y_test,predictions)
accuracy = accuracy_score(y_test, predictions)
\end{lstlisting}
The accuracy can also be found in the classification report but, accuracy\_score() is more precise. We get from these three methods all the performance measures we need, explained in section 4.5.
From trying out all the models Multinomial Naive Bayes and SVC are our best guesses to work with. But in later section we will present better option to find the most optimised model.

\subsubsection{Baseline: Sentiment classification}

In order to classify sentiment from our dataset, we used some models as a baseline and compared the results to know which method is better. For sentiment classification, we experimented between different types of learning models such as decision tree classifier, kneighbors classifier, random forest classifier, Naive Bayes model and support vector machine, using the types of text representations and features selections. We noticed that we get better results with TF-IDF vectorizer and the univariate feature selection (with Chi-square test) for our models predictions.

\subsubsection{Performance evaluation with punctuation}
For the following tables of this section, each will represent two columns with 6 rows. Each row except the first one represents, first the name of the model used and second its accuracy in the testing process.
\paragraph{Sentiment classification.}

In this section, we showcase the test accuracies for the sentiment classification. In Table \ref{tab:sentiment_without_punctuation} we trained the models without including punctuations, whereas in Table \ref{tab:sentiment_with_punctuation} we included them during the training.

\begin{table}[h!]
\centering
\caption{\scriptsize Sentiment classification test accuracy scores without punctuation.}
\label{tab:sentiment_without_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.822 \\ \hline
MultinomialNB & 0.807 \\ \hline
RandomForestClassifier & 0.800 \\ \hline
KNeighborsClassifier & 0.743 \\ \hline
DecisionTreeClassifier & 0.687 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[h!]
\centering
\caption{\scriptsize Sentiment classification test accuracy scores with punctuation.}
\label{tab:sentiment_with_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.825 \\ \hline
MultinomialNB & 0.813 \\ \hline
RandomForestClassifier & 0.804 \\ \hline
KNeighborsClassifier & 0.732 \\ \hline
DecisionTreeClassifier & 0.663 \\ \hline
\end{tabular}
%
}
\end{table}

\paragraph{Topic classification.}

In this section, we show case the test accuracies for the topic classification. In Table \ref{tab:topic_without_punctuation} we trained the models without including punctuations, whereas in Table \ref{tab:topic_with_punctuation} we included them during the training.\\

\begin{table}[h!]
\centering
\caption{\scriptsize Topic classification test accuracy scores without punctuation.}
\label{tab:topic_without_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
SVC	& 0.919 \\ \hline
MultinomialNB & 0.916 \\ \hline
KNeighborsClassifier & 0.896 \\ \hline
RandomForestClassifier & 0.880 \\ \hline
DecisionTreeClassifier & 0.798 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[h!]
\centering
\caption{\scriptsize Topic classification test accuracy scores with punctuation.}
\label{tab:topic_with_punctuation}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score \\ \hline
MultinomialNB & 0.923 \\ \hline
SVC	& 0.919 \\ \hline
KNeighborsClassifier & 0.889 \\ \hline
RandomForestClassifier & 0.885 \\ \hline
DecisionTreeClassifier & 0.797 \\ \hline
\end{tabular}
%
}
\end{table}

During this analysis, we observed that the overall performance of the models improves when including the punctuation in the review text when training the models.

\subsection{Ensemble methods}

Ensemble methods are techniques that create multiple models and then combine them to produce improved results. We can combine any machine learning algorithm such as logistic regression, decision tree, naive bayes. These models, when used as inputs of ensemble methods, are called ”base models”.

We used ensemble methods for classification and including some widely known methods of ensemble: bagging, boosting and stacking.

\subsection{Grid Search}

Grid search is an approach to find the optimal hyperparameters of a model which yields the most accurate predictions. Hyperparameter is a characteristic of an estimator which cannot be learned from data. These hyperparameters have to be set before training. During grid search, we try to find the best hyperparameters values through an exhaustive search over specified parameter values for an estimator.

For this study, we implemented a grid search framework that iterates over possible estimator candidates and executes grid search for each classifier while considering the defined hyperparameter search space.

\subsubsection{Grid Search: Base classifiers}

We executed a grid search for both sentiment and topic classification tasks. The best hyperparameters for the sentiment and topic classification task can be consulted in the Appendix in Table \ref{tab:sentiment_gs} and in Table \ref{tab:topic_gs} respectively. The \textit{best\_score} in these tables represent the training accuracies.

\subsubsection{Grid Search: Ensemble methods}

The best hyperparameters of the ensemble methods for the sentiment and topic classification task can be consulted in the Appendix in Table \ref{tab:sentiment_em_gs} and in Table \ref{tab:topic_em_gs} respectively. The \textit{best\_score} in these tables represent the training accuracies.

% }}}

% Results and discussion {{{
\section{Results \& Discussion}%Alex

%This is an important section in your report. Try to spend more time on this section.
\subsection{Results}

We can see in Table \ref{tab:sentimentassessment} and \ref{tab:topicassessment} in order the best scores for each model we have tested with the grid search. The result for both topic and sentiment classification are highest with the Stacking Classifier. These best\_scores are being extracted from the Grid Search explained before and grouped in this table with the results of the base classifiers and the ensemble methods. These results use the data set pre-processed and makes use of cross-validation with k=5. It mainly helped to find the best model to use for classifying reviews. The best\_score here represents the testing accuracy. You can find the best hyper-parameters for each model in table \ref{tab:sentimentassessment} and \ref{tab:topicassessment} in table \ref{tab:topic_gs_hp} and \ref{tab:topic_em_gs_hp} or \ref{tab:sentiment_gs_hp} and \ref{tab:sentiment_em_gs_hp} respectively. Now that we have the results of our research we can discuss on how would be the perfect model for classifying reviews according to sentiment or topic. 

% You can split \verb!Results! and \verb!Discussion! sections. Use the possibility of adding table \ref{tab:sample} and figure \ref{fig:boxplot} to make your report to be easier to follow for readers! 
% You can use following websites to make your latex table:

% \begin{itemize}
%     \item {\href{https://www.latex-tables.com/}{The first suggestion}}
%     \item {\href{https://www.tablesgenerator.com/}{The second suggestion}}
% \end{itemize}

% Figure \ref{fig:boxplot} is a sample of a figure which you can use in your report. 

% \begin{figure}[!hbtp]
%   \centering
%   \includegraphics[width = 1\linewidth, height = 0.5\linewidth]{boxplot_age_ADS.png}
%   \caption{\scriptsize \scriptsize Box plot of participants age.\label{fig:boxplot}}
% \end{figure}

\begin{table}[!ht]
\centering
\caption{\scriptsize Sentiment classification final assessment where \textit{best\_score} represents the testing accuracy. \label{tab:sentimentassessment}}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{|l|l|l|}
\hline
             & best\_score\\ \hline
\textbf{Stacking Classifier}     & \textbf{0.833}     \\ \hline
\textbf{SVC}                     & \textbf{0.825}     \\ \hline
Extra Trees Classifier  & 0.814     \\ \hline
MultinomialNB           & 0.813     \\ \hline
RandomForestClassifier  & 0.805     \\ \hline
Bagging Classifier      & 0.801     \\ \hline
XGBoost                 & 0.788     \\ \hline
Gradient Boosting       & 0.787     \\ \hline
KNeighborsClassifier    & 0.732     \\ \hline
DecisionTreeClassifier  & 0.663     \\ \hline

\end{tabular}}
\end{table}

\begin{table}[!ht]
\centering
\caption{\scriptsize Topic classification final assessment where \textit{best\_score} represents the testing accuracy. \label{tab:topicassessment}}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{|l|l|l|}
\hline
             & best\_score\\ \hline
\textbf{Stacking Classifier}     & \textbf{0.929}     \\ \hline
\textbf{MultinomialNB}           & \textbf{0.923}     \\ \hline
SVC                     & 0.919     \\ \hline
Bagging Classifier      & 0.907     \\ \hline
Extra Trees Classifier  & 0.893     \\ \hline
KNeighborsClassifier    & 0.889     \\ \hline
XGBoost                 & 0.886     \\ \hline
RandomForestClassifier  & 0.885     \\ \hline
Gradient Boosting       & 0.865     \\ \hline
DecisionTreeClassifier  & 0.797     \\ \hline

\end{tabular}}
\end{table}

\subsection{Discussion}
With the results we clearly see that Stacking Classifier would be the best option in terms of Classifier for this job. Unfortunately it has the disadvantage to require a lot of computing power. With a normal computer it would take numerous minutes to train this kind of model and test it. In case of critical works like health classification such decision would be important, but for this kind of operation SVC or MultinomialNB is probably more adapted. Classifying reviews is not as critical , thus using a lighter model in terms of computing power would be preferable. Moreover the difference here is only of +/- 0.010. \newline
Another point of discussion is the use of punctuation in data-sets like these. We can clearly imagine a upset customer leaving a review with multiple !, to show his disappointment. Therefore we decided to leave the punctuation in the data-set and not take them away during pre-processing.\newline
Overall the grid search method used during this project has offered a lot of help in finding the right model and the most suited hyper-parameters. It requires some computing power but it saves a lot of time due to the fact that we can let it run and it executes all the possibilities for us. \newline
We can imagine the final step of a project like this one as such: 
\begin{itemize}
    \item Create an SVC model with hyper-parameters taken from grid search.
    \item Use our processed data and start training the model using cross-validation.
    \item use the trained model in an back-end application to classify reviews according to their sentiment or topic, depending what you trained our model for.
\end{itemize}

% }}}

% Conclusion {{{
\section{Conclusion}

% Summary your findings in one or two paragraphs. If you have any reference to support your work, you can also add them to your report \cite{gerven1997comparative}. 
% This is a template for your report, please submit the pdf file for your report at the end. 
% The name of pdf file should be:

We presented our work on sentiment analysis for the \textit{text} project from phase 2 of the DSH course. During this project, we try to classify product reviews to a sentiment or a review topic. We achieved for sentiment and topic classification test accuracies of 0.833\% and 0.929\% respectively.

These test accuracies were obtained by the stacking classifiers. We discussed which type of classifiers should be selected and concluded that it depends on the setting in which it will be used.
% }}}

% Table of collaborations {{{
\section{Table of collaborations}

You will find the task descriptions executed by each group member in Table \ref{tab:collab} in the Appendix.

% In this section, we want you briefly describe what each one did in your group for the assigned project (Table \ref{tab:collab}).
% }}}

% % Bibliography {{{
% \bibliographystyle{apalike}
% \raggedright
% {\scriptsize
% \bibliography{ref.bib}}
% % }}}

% Appendix {{{
\onecolumn
\newpage
\clearpage
\section{Appendix}
% }}}

\subsection{Grid Search}

\subsubsection{Tuned hyperparameters of base classifiers}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the sentiment classification task.}
\label{tab:sentiment_gs_hp}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|}
\hline
classifier & best\_parameters\\ \hline
SVC & \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\} \\ \hline
MultinomialNB & \{'alpha': 10\} \\ \hline
RandomForestClassifier & \{'class\_weight': None, 'criterion': 'entropy', 'n\_estimators': 70\} \\ \hline
KNeighborsClassifier & \{'n\_neighbors': 85, 'p': 2\}\\ \hline
DecisionTreeClassifier & \{'class\_weight': 'balanced', 'criterion': 'entropy', 'splitter': 'best'\}\\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Results found during grid search for the sentiment classification task.}
\label{tab:sentiment_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|}
\hline
classifier & best\_score & std\_test\_score \\ \hline
SVC	& 0.824	& 0.022 \\ \hline
MultinomialNB & 0.807	& 0.031 \\ \hline
RandomForestClassifier & 0.802	& 0.014 \\ \hline
KNeighborsClassifier & 0.742	& 0.017 \\ \hline
DecisionTreeClassifier 	& 0.692	& 0.024 \\ \hline
\end{tabular}
%
}
\end{table}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the topic classification task.}
\label{tab:topic_gs_hp}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|}
\hline
classifier & best\_parameters\\ \hline
SVC& \{'C': 10, 'class\_weight': 'balanced', 'kernel': 'rbf', 'probability': True\} \\ \hline
MultinomialNB& \{'alpha': 1\} \\ \hline
RandomForestClassifier& \{'class\_weight': 'balanced\_subsample', 'criterion': 'gini', 'n\_estimators': 90\} \\ \hline
KNeighborsClassifier& \{'n\_neighbors': 95, 'p': 2\} \\ \hline
DecisionTreeClassifier& \{'class\_weight': None, 'criterion': 'gini', 'splitter': 'random'\} \\ \hline
\end{tabular}
%
}
\end{table}


\begin{table}[!ht]
\centering
\caption{Results found during grid search for the topic classification task.}
\label{tab:topic_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|}
\hline
classifier & best\_score & std\_test\_score \\ \hline
SVC	& 0.919	& 0.003 \\ \hline
MultinomialNB & 0.917	& 0.011 \\ \hline
KNeighborsClassifier & 0.890	& 0.008 \\ \hline
RandomForestClassifier & 0.881	& 0.008 \\ \hline
DecisionTreeClassifier & 0.800	& 0.023 \\ \hline
\end{tabular}
%
}
\end{table}

\subsubsection{Tuned hyperparameters of ensemble methods}

\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the sentiment classification task.}
\label{tab:sentiment_em_gs_hp}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_parameters \\ \hline
BaggingClassifier & \{'base\_estimator': SVC(C=10, class\_weight='balanced', probability=True), 'max\_samples': 500, 'n\_estimators': 80\}\\ \hline
ExtraTreesClassifier & \{'class\_weight': None, 'criterion': 'entropy', 'max\_samples': 400, 'n\_estimators': 90\} \\ \hline
GradientBoostingClassifier & \{'learning\_rate': 0.3, 'n\_estimators': 120\} \\ \hline
StackingClassifier 	& \{'final\_estimator': LogisticRegression(solver='liblinear')\} \\ \hline
XGBClassifier & \{'learning\_rate': 0.3, 'n\_estimators': 130\} \\ \hline
\end{tabular}
%
}
\end{table}


\begin{table}[!ht]
\centering
\caption{Results found during grid search for the sentiment classification task.}
\label{tab:sentiment_em_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score & std\_test\_score \\ \hline
BaggingClassifier & 0.807	& 0.031 \\ \hline
ExtraTreesClassifier & 0.820	& 0.022 \\ \hline
GradientBoostingClassifier & 0.784 & 0.013 \\ \hline
StackingClassifier 	& 0.822	& 0.019 \\ \hline
XGBClassifier & 0.794 & 0.023 \\ \hline
\end{tabular}
%
}
\end{table}


\begin{table}[!ht]
\centering
\caption{Best hyperparameters found during grid search for the topic classification task.}
\label{tab:topic_em_gs_hp}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_parameters \\ \hline
BaggingClassifier & \{base\_estimator=SVC(C=10, class\_weight='balanced', probability=True), max\_samples=500, n\_estimators=80\} \\ \hline
ExtraTreesClassifier & \{'class\_weight': 'balanced', 'criterion': 'gini', 'max\_samples': 200, 'n\_estimators': 90\} \\ \hline
GradientBoostingClassifier & \{'learning\_rate': 0.2, 'n\_estimators': 120\} \\ \hline
StackingClassifier 	& \{'final\_estimator': LogisticRegression(solver='liblinear')\} \\ \hline
XGBClassifier & \{'learning\_rate': 0.3, 'n\_estimators': 130\} \\ \hline
\end{tabular}
%
}
\end{table}


\begin{table}[!ht]
\centering
\caption{Results found during grid search for the topic classification task.}
\label{tab:topic_em_gs}
\small\addtolength{\tabcolsep}{-4pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
classifier & best\_score & std\_test\_score \\ \hline
BaggingClassifier & 0.907	& 0.009 \\ \hline
ExtraTreesClassifier & 0.898	& 0.014 \\ \hline
GradientBoostingClassifier & 0.861	& 0.015 \\ \hline
StackingClassifier & 0.925 & 0.006 \\ \hline
XGBClassifier & 0.886	& 0.016 \\ \hline
\end{tabular}
%
}
\end{table}

\newpage

\subsection{Task descriptions}

% Task descriptions {{{
\begin{table}[!h]
\centering
\caption{\scriptsize Task descriptions executed by each student of the group.}\label{tab:collab}
\begin{tabular}{|c| >{\centering\arraybackslash}m{0.6\textwidth}|}\hline

\textbf{Group member}  & \textbf{Task} \\

\hline

\multirow{2}{*}{\textbf{Leminh Nguyen}}

                  & Helping out with implementing the data visualisation \\\cline{2-2}
                  & Annotate the notebooks with small descriptions. \\\cline{2-2}
                  & Implementation of dataset conversion to Pandas DataFrame representation \\\cline{2-2}
                  & Implementation of data pre-processing (Data cleaning, selection, normalization) \\\cline{2-2}
                  & Define performance measure + cross validation \\\cline{2-2}
                  & Definition of Grid Search and implementation of grid search framework to iterate over a set of classifer-hyperparameters candidates \\\cline{2-2}
                  & Performance evaluation with punctuation \\\cline{2-2}
                  & Performance evaluation without punctuation \\\cline{2-2}
                  & Sentiment Grid search benchmark \\\cline{2-2}
                  & Topic Grid search benchmark \\\cline{2-2}
                  % & Final benchmark assessment \\\cline{2-2}
                  % & Best model selection \\\cline{2-2}
                  % & Formulate abstract, introduction, discussion and conclusion sections \\\cline{2-2}
                  & Formulate abstract, introduction conclusion sections \\\cline{2-2}
                  & Format the report template \\

\hline

\multirow{2}{*}{\textbf{Alex Poldrugo}}

                  & Implementing the data visualisation.\\\cline{2-2}
                  & Perform the review to topic classification.\\\cline{2-2}
                  & Include KNN model to our research.\\\cline{2-2}
                  & Formulate Baseline model, NB and SVC model explanation and visualisation of data section.\\\cline{2-2}
                  & Formulate Result and Discussion section.\\\cline{2-2}
                  & Code comparison.\\

\hline

\multirow{2}{*}{\textbf{Rafidison Rakotondrasoa}}

                  & Implementing two types of text representations  \\\cline{2-2}
                  & Implementing two types of feature selections \\\cline{2-2}
                  & Using our baseline model, comparison of the results for sentiment classification with the text representation and feature selection methods.  \\\cline{2-2}
                  & Making research on ensemble methods including the different types of algorithms and its implementations. 
                  \\\cline{2-2}
                  & GridSearch for the ensemble method models.
                  \\

\hline

\end{tabular}
\end{table}
% }}}

% Comments {{{
% \section{Pdf}
% Please save your file as a pdf file and apply the following template for naming your pdf file.
% \textit{DSH\_nameofyourproject\_phasenumber}
% \noindent Good luck :)
% }}}

\end{document}
